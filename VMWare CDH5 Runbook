UI PORTS
192.168.38.143:50070 - NN UI
192.168.38.142:8088 - YARN
192.168.38.143:19888 - Job history

##### BASE ######
vim /etc/selinux/config 
# (SELINUX=disabled)
service iptables stop && chkconfig iptables off
/etc/hosts fdqn and hostname
service network restart

yum -y install openssh-server vim

#Install VMWseARE Tools

chkconfig sshd on

vim /etc/yum.repos.d/cloudera-cdh5.repo
[cloudera-cdh5]
# Packages for Cloudera's Distribution for Hadoop, Version 5, on RedHat	or CentOS 6 x86_64
name=Cloudera's Distribution for Hadoop, Version 5
baseurl=http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5/
gpgkey = http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera    
gpgcheck = 1

yum update

# JAVA
rpm -Uvh <JAVA>
alternatives --config java
export JAVA_HOME=/usr/java/jdk1.8.0_65
export PATH=$PATH:$JAVA_HOME/bin

service sshd start
#### BASE ####

#### Clone 
master
192.168.38.143
slave
192.168.38.142

192.168.38.143
192.168.38.142

## Configure SSH
ssh-keygen -t rsa -P ""
cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
auth 600
ssh-copy-id -i $HOME/.ssh/id_rsa.pub root@slave

#Install ZK (ROOT)
yum -y install zookeeper-server
mkdir -p /var/lib/zookeeper
chown -R zookeeper /var/lib/zookeeper/
service zookeeper-server init
service zookeeper-server start

# Master Machine
yum -y install hadoop-hdfs-namenode hadoop-mapreduce-historyserver hadoop-yarn-proxyserver hadoop-client

# Slave Machine
yum -y install hadoop-hdfs-secondarynamenode hadoop-yarn-resourcemanager hadoop-yarn-nodemanager hadoop-hdfs-datanode hadoop-mapreduce hadoop-client


cd /etc/hadoop/conf
# Core Site xml
vim /etc/hadoop/conf/core-site.xml
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://master.hadoop.com:8020</value>
</property>

#HDFS
<property>
	<name>dfs.permissions.superusergroup</name>
	<value>hadoop</value>
</property>
<property>
	<name>dfs.replication</name>
	<value>2</value>
</property>
<property>
	<name>dfs.namenode.http-address</name>
	<value>master.hadoop.com:50070</value>
</property>


mkdir -p /var/lib/hadoop-hdfs/cache/hdfs/dfs/name
chown -R hdfs:hdfs /var/lib/hadoop-hdfs/cache/hdfs/dfs/name
chmod 700 /var/lib/hadoop-hdfs/cache/hdfs/dfs/name
su hdfs hdfs namenode -format

## DFS Service
for x in `cd /etc/init.d ; ls hadoop-hdfs-*` ; do service $x start ; done
killall java


sudo -u hdfs hadoop fs -mkdir /user/hadoop
sudo -u hdfs hadoop fs -chown hadoop /user/hadoop

##############################################


--- Non Slides
## YARN ##
core-site.xml
<property>
<name>hadoop.proxyuser.mapred.groups</name>
<value>*</value>
</property>
<property>
<name>hadoop.proxyuser.mapred.hosts</name>
<value>*</value>
</property>



mapred-site.xml

<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
<property>
<name>yarn.app.mapreduce.am.staging-dir</name>
<value>/user</value>
</property>
<property>
<name>mapreduce.jobhistory.address</name>
<value>master.hadoop.com:10020</value>
</property>
<property>
<name>mapreduce.jobhistory.webapp.address</name>
<value>master.hadoop.com:19888</value>
</property>

yarn-site.xml
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.log-aggregation-enable</name>
<value>true</value>
</property>
<property>
<description>List of directories to store localized files in.</description>
<name>yarn.nodemanager.local-dirs</name>
<value>file:///var/lib/hadoop-yarn/cache/${user.name}/nm-local-dir</value>
</property>
<property>
<description>Where to store container logs.</description>
<name>yarn.nodemanager.log-dirs</name>
<value>file:///var/log/hadoop-yarn/containers</value>
</property>
<property>
<description>Where to aggregate logs to.</description>
<name>yarn.nodemanager.remote-app-log-dir</name>
<value>hdfs://master.hadoop.com:8020/var/log/hadoop-yarn/apps</value>
</property>
<property>
<description>Classpath for typical applications.</description>
<name>yarn.application.classpath</name>
<value>
$HADOOP_CONF_DIR,
$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
</value>
</property>
<property>
<name>yarn.resourcemanager.hostname</name>
<value>slave.hadoop.com</value>
</property>
</configuration>


############## YARN SERVICES #######################
## Master
sudo service hadoop-mapreduce-historyserver start

## Slave
sudo service hadoop-yarn-resourcemanager start
sudo service hadoop-yarn-nodemanager start

http://crazyadmins.com/install-multinode-cloudera-hadoop-cluster-cdh5-4-0-manually/

#download book
wget http://www.gutenberg.org/cache/epub/20417/pg20417.txt

# Installing PIG
yum install pig

a = load 'book';
b = foreach a generate flatten(TOKENIZE((chararray)$0)) as word;
c = group b by word;
d = foreach c generate COUNT(b), group;
dump d;

